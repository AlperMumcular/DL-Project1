{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is after we got a 0.78 in the last run on 8th march.\n",
        "MODEL"
      ],
      "metadata": {
        "id": "X1yMB-wEN6Uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model got 0.84517"
      ],
      "metadata": {
        "id": "eGc6ZjrGdkTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "#                       Squeeze-and-Excitation\n",
        "# ------------------------------------------------------------------------\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        # Squeeze: global spatial average pooling\n",
        "        squeeze = x.view(b, c, -1).mean(dim=2)\n",
        "        # Excitation: two FC layers with ReLU and Sigmoid activations\n",
        "        excitation = F.relu(self.fc1(squeeze))\n",
        "        excitation = torch.sigmoid(self.fc2(excitation)).view(b, c, 1, 1)\n",
        "        return x * excitation\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "#                       Stochastic Depth\n",
        "# ------------------------------------------------------------------------\n",
        "class StochasticDepth(nn.Module):\n",
        "    \"\"\"Drops residual branch with probability p.\"\"\"\n",
        "    def __init__(self, p: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        if not self.training or self.p == 0.0:\n",
        "            return x + residual\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return x\n",
        "        else:\n",
        "            return x + residual\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "#       PreAct Residual Block with SE + StochasticDepth\n",
        "# ------------------------------------------------------------------------\n",
        "class PreActBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride=1, drop_prob=0.0):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.se = SEBlock(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.shortcut = None\n",
        "        if stride != 1 or in_planes != out_planes:\n",
        "            self.shortcut = nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                                      stride=stride, padding=0, bias=False)\n",
        "\n",
        "        # Stochastic Depth probability\n",
        "        self.sd = StochasticDepth(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        out = self.se(out)  # Squeeze-Excitation on pre-activated features\n",
        "        shortcut = x if self.shortcut is None else self.shortcut(out)\n",
        "        out = self.conv1(out)\n",
        "        out = F.relu(self.bn2(out))\n",
        "        out = self.conv2(out)\n",
        "        return self.sd(shortcut, out)\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "#       Modified LightResNet for CIFAR-10 with ~4.4M parameters\n",
        "#       (Channels: 40, 80, 160, 320)\n",
        "# ------------------------------------------------------------------------\n",
        "class LightResNet18_v2(nn.Module):\n",
        "    def __init__(self, num_classes=10, drop_prob=0.1):\n",
        "        super().__init__()\n",
        "        # Increase the base channel width to 40 (instead of 32)\n",
        "        self.in_planes = 45\n",
        "        self.conv1 = nn.Conv2d(3, 45, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "\n",
        "        # Four layers with 2 blocks each; channels increase as 40 -> 80 -> 160 -> 320.\n",
        "        self.layer1 = self._make_layer(45, 2, stride=1, base_p=drop_prob * 1/4)\n",
        "        self.layer2 = self._make_layer(90, 2, stride=2, base_p=drop_prob * 2/4)\n",
        "        self.layer3 = self._make_layer(170, 2, stride=2, base_p=drop_prob * 3/4)\n",
        "        self.layer4 = self._make_layer(340, 2, stride=2, base_p=drop_prob * 4/4)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(340)\n",
        "        self.linear = nn.Linear(340, num_classes)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, out_planes, blocks, stride, base_p):\n",
        "        strides = [stride] + [1] * (blocks - 1)\n",
        "        layers = []\n",
        "        for i in range(blocks):\n",
        "            # Increase dropout probability linearly across blocks\n",
        "            block_p = base_p * (i + 1) / blocks\n",
        "            layers.append(PreActBlock(self.in_planes, out_planes,\n",
        "                                      stride=strides[i],\n",
        "                                      drop_prob=block_p))\n",
        "            self.in_planes = out_planes\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.relu(self.bn(out))\n",
        "        out = F.adaptive_avg_pool2d(out, 1).view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Quick parameter check\n",
        "if __name__ == \"__main__\":\n",
        "    model = LightResNet18_v2(num_classes=10, drop_prob=0.1)\n",
        "    x = torch.randn(2, 3, 32, 32)\n",
        "    y = model(x)\n",
        "    print(\"Output shape:\", y.shape)\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total trainable parameters: {total_params:,} (~{total_params/1e6:.2f}M)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X70034zUlMmV",
        "outputId": "431e6fc2-a4f2-4086-8cd1-6d84289c1f50"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 10])\n",
            "Total trainable parameters: 4,998,947 (~5.00M)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "#from model import LightResNet18\n",
        "from tqdm import tqdm\n",
        "\n",
        "# We'll define a custom collate function for MixUp/CutMix\n",
        "# randomly picks one method for each batch\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[3]\n",
        "    H = size[2]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "    return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size)\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)\n",
        "    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size(-1) * x.size(-2)))\n",
        "    y_a, y_b = y, y[index]\n",
        "    return x, y_a, y_b, lam\n",
        "\n",
        "def mixup_cutmix_collate(batch, alpha=1.0, p=0.5):\n",
        "    # Standard collate\n",
        "    images, labels = list(zip(*batch))\n",
        "    images = torch.stack(images, 0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Flip a coin to choose MixUp or CutMix\n",
        "    if np.random.rand() < p:\n",
        "        # MixUp\n",
        "        mixed_x, y_a, y_b, lam = mixup_data(images, labels, alpha)\n",
        "        return mixed_x, (y_a, y_b, lam, 'mixup')\n",
        "    else:\n",
        "        # CutMix\n",
        "        cutmix_x, y_a, y_b, lam = cutmix_data(images, labels, alpha)\n",
        "        return cutmix_x, (y_a, y_b, lam, 'cutmix')\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        data_dict = pickle.load(fo, encoding='bytes')\n",
        "    return data_dict\n",
        "\n",
        "class CIFARDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data.reshape(-1, 3, 32, 32).astype(\"float32\") / 255.0\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]*255\n",
        "        img = img.astype(\"uint8\")\n",
        "        # shape: (3,32,32) => for PIL => (32,32,3)\n",
        "        img = Image.fromarray(img.transpose(1,2,0))\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "def load_cifar10_batches(root_dir):\n",
        "    data_list, labels_list = [], []\n",
        "    for i in range(1, 6):\n",
        "        batch_file = f\"{root_dir}/data_batch_{i}\"\n",
        "        batch = unpickle(batch_file)\n",
        "        data_list.append(batch[b'data'])\n",
        "        labels_list.extend(batch[b'labels'])\n",
        "    X = np.concatenate(data_list, axis=0)\n",
        "    y = np.array(labels_list)\n",
        "    return X, y\n",
        "\n",
        "def train():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 2) Transforms: RandAugment or AutoAugment\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), value='random')\n",
        "    ])\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ])\n",
        "\n",
        "    #train_dataset = CIFARDataset(X_train, y_train, transform=transform_train)\n",
        "    #val_dataset   = CIFARDataset(X_val,   y_val,   transform=transform_val)\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                 download=True, transform=transform_train)\n",
        "    val_dataset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                download=True, transform=transform_val)\n",
        "    # Collate with MixUp & CutMix\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128,\n",
        "                              shuffle=True, num_workers=4,\n",
        "                              collate_fn=lambda b: mixup_cutmix_collate(b, alpha=1.0, p=0.5))\n",
        "    val_loader   = DataLoader(val_dataset,   batch_size=128,\n",
        "                              shuffle=False, num_workers=4)\n",
        "\n",
        "    # 3) Initialize model\n",
        "    model = LightResNet18_v2(num_classes=10, drop_prob=0.1).to(device)\n",
        "\n",
        "    # 4) Loss, Optimizer, Scheduler\n",
        "    # We'll handle MixUp/CutMix label logic manually, but also do label_smoothing\n",
        "    base_criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    num_epochs = 300\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_approx = 0.0\n",
        "        total_approx = 0\n",
        "\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
        "        for images, label_info in train_pbar:\n",
        "            # label_info = (y_a, y_b, lam, method)\n",
        "            y_a, y_b, lam, method = label_info\n",
        "            images = images.to(device)\n",
        "            y_a = y_a.to(device)\n",
        "            y_b = y_b.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            # MixUp/CutMix combined loss\n",
        "            loss = lam * base_criterion(outputs, y_a) + (1 - lam) * base_criterion(outputs, y_b)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            # approximate accuracy counting\n",
        "            correct_approx += (preds == y_a).sum().item() * lam + (preds == y_b).sum().item() * (1 - lam)\n",
        "            total_approx   += images.size(0)\n",
        "\n",
        "            train_pbar.set_postfix(loss=f\"{loss.item():.3f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc  = 100.0 * correct_approx / total_approx\n",
        "\n",
        "        # ----- Validation -----\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                _, pred = torch.max(outputs, 1)\n",
        "                val_correct += (pred == labels).sum().item()\n",
        "                val_total   += labels.size(0)\n",
        "        val_acc = 100.0 * val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Approx Train Acc: {epoch_acc:.2f}% | \"\n",
        "              f\"Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(f\"   [*] Saved new best model: Val Acc = {val_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Training complete. Best validation accuracy: {best_acc:.2f}%\")\n",
        "    print(\"Best model saved as best_model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "wZcL7rFHlO95",
        "outputId": "6d64a03d-c130-4a85-eed7-3172f322b60f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300 | Train Loss: 2.1575, Approx Train Acc: 21.78% | Val Acc: 39.47%\n",
            "   [*] Saved new best model: Val Acc = 39.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/300 | Train Loss: 1.9978, Approx Train Acc: 32.19% | Val Acc: 54.28%\n",
            "   [*] Saved new best model: Val Acc = 54.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1c4ced0ff873>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-1c4ced0ff873>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mtrain_pbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{num_epochs}]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_pbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;31m# label_info = (y_a, y_b, lam, method)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0my_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m'''Receive a handle over a local connection.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecvfds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDupFd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mbytes_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMSG_SPACE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference.py\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "#from model import LightResNet18\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        data_dict = pickle.load(fo, encoding='bytes')\n",
        "    return data_dict\n",
        "\n",
        "class CIFARTestDataset(Dataset):\n",
        "    def __init__(self, data, ids, transform=None):\n",
        "        \"\"\"\n",
        "        data: shape (N, 32, 32, 3)\n",
        "        ids: array/list of image IDs\n",
        "        transform: transforms to apply\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.ids = ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx].astype(\"uint8\")  # ensure 0..255\n",
        "        img_id = self.ids[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, img_id\n",
        "\n",
        "def inference():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 1) Load the best model\n",
        "    model = LightResNet18_v2(num_classes=10, drop_prob=0.1).to(device)\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 2) Load custom test set .pkl\n",
        "    test_file = \"/content/cifar_test_nolabel.pkl\"\n",
        "    test_dict = unpickle(test_file)\n",
        "    print(\"Keys in test_dict:\", test_dict.keys())\n",
        "\n",
        "    # Reshape if it's (N, 3072). If it's already (N, 32,32,3), remove reshape\n",
        "    test_images = test_dict[b'data'].reshape(-1, 32, 32, 3)\n",
        "    test_ids = [str(i) for i in range(len(test_images))]\n",
        "\n",
        "    # 3) Define test transforms\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ])\n",
        "\n",
        "    test_dataset = CIFARTestDataset(test_images, test_ids, transform=transform_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
        "\n",
        "    # 4) Inference with 2-pass TTA\n",
        "    predictions = []\n",
        "    image_ids = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, ids in test_loader:\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            # Pass 1: Normal\n",
        "            out_normal = model(imgs)\n",
        "            probs_normal = F.softmax(out_normal, dim=1)\n",
        "\n",
        "            # Pass 2: Horizontal flip\n",
        "            imgs_flipped = torch.flip(imgs, dims=[3])  # flip W dimension\n",
        "            out_flipped = model(imgs_flipped)\n",
        "            probs_flipped = F.softmax(out_flipped, dim=1)\n",
        "\n",
        "            # Average probabilities\n",
        "            final_probs = (probs_normal + probs_flipped) / 2.0\n",
        "            _, predicted = torch.max(final_probs, 1)\n",
        "\n",
        "            predictions.extend(predicted.cpu().numpy().tolist())\n",
        "            image_ids.extend(ids)\n",
        "\n",
        "    # 5) Save submission\n",
        "    submission_df = pd.DataFrame({\"ID\": image_ids, \"Labels\": predictions})\n",
        "    submission_df.to_csv(\"submission.csv\", index=False)\n",
        "    print(\"Submission file saved as submission.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inference()\n"
      ],
      "metadata": {
        "id": "jQ7SyXrHlT9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7216e473-8469-4eb3-d8f9-5cab99e4e09b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Keys in test_dict: dict_keys([b'data', b'ids'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-15bfa4ed092b>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved as submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load and inspect submission file\n",
        "submission_df = pd.read_csv(\"submission.csv\")\n",
        "print(submission_df.head())  # Show first few rows\n",
        "print(submission_df[\"Labels\"].value_counts())  # Show label distribution\n"
      ],
      "metadata": {
        "id": "p9KKCKOR1g1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd28727b-3bc8-45bc-86fb-400675ecf9b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID  Labels\n",
            "0   0       6\n",
            "1   1       1\n",
            "2   2       8\n",
            "3   3       6\n",
            "4   4       9\n",
            "Labels\n",
            "1    1062\n",
            "3    1062\n",
            "5    1054\n",
            "8    1050\n",
            "7    1036\n",
            "9     981\n",
            "4     974\n",
            "6     953\n",
            "2     946\n",
            "0     882\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}